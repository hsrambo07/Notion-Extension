import { GoogleGenerativeAI } from '@google/generative-ai';
import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { BedrockRuntimeClient, ConverseCommand } from '@aws-sdk/client-bedrock-runtime';
import { fromIni } from '@aws-sdk/credential-providers';
import { v4 } from 'uuid';
import Ajv from 'ajv';

// src/llms/gemini.ts

// src/utils/tokenCounter.ts
var MODEL_COSTS = {
  // GPT-4 Models
  "gpt-4-0125-preview": { input: 10, output: 30 },
  "gpt-4-1106-preview": { input: 10, output: 30 },
  "gpt-4-vision-preview": { input: 10, output: 30 },
  "gpt-4": { input: 30, output: 60 },
  "gpt-4-32k": { input: 60, output: 120 },
  "gpt-4o": { input: 250, output: 1e3 },
  "gpt-4o-mini": { input: 15, output: 60 },
  // GPT-3.5 Models
  "gpt-3.5-turbo-0125": { input: 0.5, output: 1.5 },
  "gpt-3.5-turbo-instruct": { input: 1.5, output: 2 },
  "gpt-3.5-turbo-16k": { input: 0.5, output: 1.5 },
  "gpt-3.5-turbo": { input: 0.5, output: 1.5 },
  // Claude Models
  "claude-3-opus-20240229": { input: 15, output: 75 },
  "claude-3-sonnet-20240229": { input: 3, output: 15 },
  "claude-3-haiku-20240307": { input: 0.25, output: 1.25 },
  "claude-2.1": { input: 8, output: 24 },
  "claude-2.0": { input: 8, output: 24 },
  "claude-instant-1.2": { input: 0.8, output: 2.4 },
  // Cloudflare AI Models
  "@cf/meta/llama-2-7b-chat-int8": { input: 0.2, output: 0.4 },
  "@cf/meta/llama-2-13b-chat-int8": { input: 0.4, output: 0.8 },
  "@cf/meta/llama-2-70b-chat-int8": { input: 1, output: 2 },
  "@cf/mistral/mistral-7b-instruct-v0.1": { input: 0.2, output: 0.4 },
  "@cf/tiiuae/falcon-7b-instruct": { input: 0.2, output: 0.4 },
  "@cf/anthropic/claude-instant-1.2": { input: 0.8, output: 2.4 },
  "@cf/anthropic/claude-2.1": { input: 8, output: 24 },
  "custom-http-model": { input: 0.5, output: 1.5 }
};
function isKnownModel(model) {
  return model in MODEL_COSTS;
}
function calculateCost(inputTokens, outputTokens, model) {
  if (!isKnownModel(model)) {
    model = "gpt-3.5-turbo";
  }
  const costs = MODEL_COSTS[model];
  const inputCost = inputTokens / 1e6 * costs.input;
  const outputCost = outputTokens / 1e6 * costs.output;
  return Number((inputCost + outputCost).toFixed(6));
}

// src/llms/gemini.ts
function createGeminiLLM(config) {
  const client = new GoogleGenerativeAI(config.apiKey);
  const defaultModel = "gemini-2.0-flash";
  const model = config.model || defaultModel;
  return {
    modelName: model,
    async complete({
      prompt,
      schema,
      temperature,
      maxTokens
    }) {
      var _a, _b, _c, _d, _e, _f, _g, _h;
      const generativeModel = client.getGenerativeModel({ model });
      const data = await generativeModel.generateContent({
        contents: [
          {
            role: "user",
            parts: [
              {
                text: schema ? `Respond only with a JSON object matching this schema:
${JSON.stringify(
                  schema,
                  null,
                  2
                )}

${prompt}` : prompt
              }
            ]
          }
        ],
        generationConfig: {
          temperature: temperature != null ? temperature : 0.7,
          maxOutputTokens: maxTokens
        }
      });
      const candidate = (_a = data.response.candidates) == null ? void 0 : _a[0];
      if (!candidate) throw new Error("No response from Gemini");
      let rawOutput = ((_d = (_c = (_b = candidate.content) == null ? void 0 : _b.parts) == null ? void 0 : _c[0]) == null ? void 0 : _d.text) || "";
      rawOutput = rawOutput.replace(/^\s*```(?:json)?|```\s*$/g, "").trim();
      let content;
      if (schema) {
        const parsedOutput = JSON.parse(rawOutput);
        if (typeof parsedOutput !== "object" || !parsedOutput) {
          throw new Error("Parsed response does not match schema");
        }
        content = parsedOutput;
      } else {
        content = rawOutput;
      }
      return {
        content,
        inputTokens: ((_e = data.response.usageMetadata) == null ? void 0 : _e.promptTokenCount) || 0,
        outputTokens: ((_f = data.response.usageMetadata) == null ? void 0 : _f.candidatesTokenCount) || 0,
        costCents: calculateCost(
          ((_g = data.response.usageMetadata) == null ? void 0 : _g.promptTokenCount) || 0,
          ((_h = data.response.usageMetadata) == null ? void 0 : _h.candidatesTokenCount) || 0,
          model
        ),
        rawInput: prompt,
        rawOutput
      };
    }
  };
}
function createOpenAILLM(config) {
  const client = new OpenAI({ apiKey: config.apiKey });
  const defaultModel = "gpt-4-turbo-preview";
  const model = config.model || defaultModel;
  return {
    modelName: model,
    async complete({
      prompt,
      schema,
      temperature,
      maxTokens
    }) {
      const response = await client.chat.completions.create({
        model,
        messages: [{ role: "user", content: prompt }],
        temperature: temperature != null ? temperature : 0.7,
        max_tokens: maxTokens,
        ...schema && {
          response_format: { type: "json_object" },
          functions: [
            {
              name: "format_response",
              description: "Format the response according to schema",
              parameters: schema
            }
          ],
          function_call: { name: "format_response" }
        }
      });
      const choice = response.choices[0];
      let content;
      let rawOutput;
      if (schema) {
        const functionCall = choice.message.function_call;
        if (!(functionCall == null ? void 0 : functionCall.arguments)) {
          throw new Error("Expected function call response");
        }
        content = JSON.parse(functionCall.arguments);
        rawOutput = functionCall.arguments;
      } else {
        content = choice.message.content;
        rawOutput = choice.message.content || "";
      }
      if (!response.usage) {
        throw new Error("No usage data in OpenAI response");
      }
      return {
        content,
        inputTokens: response.usage.prompt_tokens,
        outputTokens: response.usage.completion_tokens,
        costCents: calculateCost(
          response.usage.prompt_tokens,
          response.usage.completion_tokens,
          model
        ),
        rawInput: prompt,
        rawOutput
      };
    }
  };
}
function createAnthropicLLM(config) {
  if (!config.apiKey) {
    throw new Error(
      "Anthropic API key is required. Please set ANTHROPIC_API_KEY in your environment variables."
    );
  }
  const anthropic = new Anthropic({
    apiKey: config.apiKey
  });
  const defaultModel = "claude-3-opus-20240229";
  const model = config.model || defaultModel;
  return {
    modelName: model,
    async complete({
      prompt,
      schema,
      temperature,
      maxTokens
    }) {
      const response = await anthropic.messages.create({
        model,
        messages: [
          {
            role: "user",
            content: schema ? `You must respond ONLY with a valid JSON object matching this schema:
${JSON.stringify(schema, null, 2)}

${prompt}` : prompt
          }
        ],
        temperature: temperature != null ? temperature : 0.7,
        max_tokens: maxTokens || 1024
      });
      if (response.content[0].type !== "text") {
        throw new Error("Expected text response from Claude");
      }
      const rawOutput = response.content[0].text;
      let content;
      if (schema) {
        try {
          content = JSON.parse(rawOutput);
        } catch (e) {
          throw new Error(`Failed to parse JSON response: ${rawOutput}`);
        }
      } else {
        content = rawOutput;
      }
      return {
        content,
        inputTokens: response.usage.input_tokens,
        outputTokens: response.usage.output_tokens,
        costCents: calculateCost(
          response.usage.input_tokens,
          response.usage.output_tokens,
          model
        ),
        rawInput: prompt,
        rawOutput
      };
    }
  };
}
function createXAILLM(config) {
  if (!config.apiKey) {
    throw new Error("xAI API key is required");
  }
  const client = new OpenAI({
    apiKey: config.apiKey,
    baseURL: "https://api.x.ai/v1"
  });
  const defaultModel = "grok-2-1212";
  const model = config.model || defaultModel;
  return {
    modelName: model,
    async complete({
      prompt,
      schema,
      temperature,
      maxTokens
    }) {
      var _a, _b, _c, _d;
      try {
        const messages = [
          ...schema ? [
            {
              role: "system",
              content: `Respond with a JSON object matching this schema: ${JSON.stringify(
                schema,
                null,
                2
              )}`
            }
          ] : [],
          { role: "user", content: prompt }
        ];
        const response = await client.chat.completions.create({
          model,
          messages,
          temperature: temperature != null ? temperature : 0.7,
          max_tokens: maxTokens,
          response_format: schema ? { type: "json_object" } : void 0
        });
        const choice = response.choices[0];
        const rawOutput = choice.message.content || "";
        let content;
        if (schema) {
          try {
            content = JSON.parse(rawOutput);
          } catch (e) {
            throw new Error(
              `Failed to parse JSON response: ${rawOutput} error: ${e}`
            );
          }
        } else {
          content = rawOutput;
        }
        return {
          content,
          inputTokens: ((_a = response.usage) == null ? void 0 : _a.prompt_tokens) || 0,
          outputTokens: ((_b = response.usage) == null ? void 0 : _b.completion_tokens) || 0,
          costCents: calculateCost(
            ((_c = response.usage) == null ? void 0 : _c.prompt_tokens) || 0,
            ((_d = response.usage) == null ? void 0 : _d.completion_tokens) || 0,
            model
          ),
          rawInput: prompt,
          rawOutput
        };
      } catch (error) {
        console.error("xAI completion error:", error);
        throw error;
      }
    }
  };
}

// src/llms/cloudflare.ts
function createCloudflareAILLM(config) {
  if (!config.apiToken) {
    throw new Error(
      "Cloudflare API token is required. Please set CLOUDFLARE_API_TOKEN in your environment variables."
    );
  }
  if (!config.accountId) {
    throw new Error(
      "Cloudflare Account ID is required. Please set CLOUDFLARE_ACCOUNT_ID in your environment variables."
    );
  }
  const defaultModel = "@cf/meta/llama-2-7b-chat-int8";
  const model = config.model || defaultModel;
  return {
    modelName: model,
    async complete({
      prompt,
      schema,
      temperature,
      maxTokens
    }) {
      const response = await fetch(
        `https://api.cloudflare.com/client/v4/accounts/${config.accountId}/ai/run/${model}`,
        {
          method: "POST",
          headers: {
            Authorization: `Bearer ${config.apiToken}`,
            "Content-Type": "application/json",
            Accept: "application/json"
          },
          body: JSON.stringify({
            messages: [
              ...schema ? [
                {
                  role: "system",
                  content: `Do not include any explanation or text at the end and only respond with a JSON object matching this schema:
${JSON.stringify(
                    schema,
                    null,
                    2
                  )}`
                }
              ] : [],
              { role: "user", content: prompt }
            ],
            temperature: temperature != null ? temperature : 0.7,
            max_tokens: maxTokens || 1024
          })
        }
      );
      if (!response.ok) {
        throw new Error(
          `Cloudflare AI request failed: ${response.status} ${response.statusText}`
        );
      }
      const result = await response.json();
      const rawOutput = result.result.response;
      let content = rawOutput;
      if (schema) {
        content = JSON.parse(rawOutput);
      }
      const inputChars = prompt.length;
      const outputChars = JSON.stringify(content).length;
      const estimatedInputTokens = Math.ceil(inputChars / 4);
      const estimatedOutputTokens = Math.ceil(outputChars / 4);
      return {
        content,
        inputTokens: estimatedInputTokens,
        outputTokens: estimatedOutputTokens,
        costCents: calculateCost(
          estimatedInputTokens,
          estimatedOutputTokens,
          model
        ),
        rawInput: prompt,
        rawOutput
      };
    }
  };
}

// src/llms/http.ts
function createHttpLLM(config) {
  if (!config.endpoint) {
    throw new Error("HTTP endpoint is required");
  }
  const model = "custom-http-model";
  return {
    modelName: model,
    async complete({
      prompt,
      schema,
      temperature,
      maxTokens
    }) {
      var _a, _b, _c;
      const defaultBody = {
        messages: [
          ...schema ? [
            {
              role: "system",
              content: `Respond only with a JSON object matching this schema:
${JSON.stringify(
                schema,
                null,
                2
              )}`
            }
          ] : [],
          { role: "user", content: prompt }
        ],
        temperature: temperature != null ? temperature : 0.7,
        max_tokens: maxTokens || 1024
      };
      const requestBody = config.transformRequest ? config.transformRequest(defaultBody) : defaultBody;
      const response = await fetch(config.endpoint, {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          ...config.apiKey && { Authorization: `Bearer ${config.apiKey}` },
          ...config.headers
        },
        body: JSON.stringify(requestBody)
      });
      if (!response.ok) {
        throw new Error(
          `HTTP LLM request failed: ${response.status} ${response.statusText}`
        );
      }
      const result = await response.json();
      const rawOutput = config.transformResponse ? config.transformResponse(result) : result.response || ((_c = (_b = (_a = result.choices) == null ? void 0 : _a[0]) == null ? void 0 : _b.message) == null ? void 0 : _c.content) || result.text;
      let content = rawOutput;
      if (schema) {
        content = JSON.parse(rawOutput);
      }
      const inputChars = prompt.length;
      const outputChars = JSON.stringify(content).length;
      const estimatedInputTokens = Math.ceil(inputChars / 4);
      const estimatedOutputTokens = Math.ceil(outputChars / 4);
      return {
        content,
        inputTokens: estimatedInputTokens,
        outputTokens: estimatedOutputTokens,
        costCents: calculateCost(
          estimatedInputTokens,
          estimatedOutputTokens,
          model
        ),
        rawInput: prompt,
        rawOutput
      };
    }
  };
}
function createBedrockLLM(config) {
  const credentials = config.access_key && config.secret_key ? {
    accessKeyId: config.access_key,
    secretAccessKey: config.secret_key
  } : config.profile ? fromIni({ profile: config.profile }) : void 0;
  const bedrock = new BedrockRuntimeClient({
    region: config.region,
    profile: config.profile,
    credentials
  });
  const defaultModel = "anthropic.claude-3-5-sonnet-20240620-v1:0";
  const model = config.model || defaultModel;
  return {
    modelName: model,
    async complete({
      prompt,
      schema,
      temperature,
      maxTokens
    }) {
      var _a, _b, _c, _d, _e, _f, _g, _h, _i;
      const response = await bedrock.send(new ConverseCommand({
        modelId: model,
        messages: [{ role: "user", content: [{ text: prompt }] }],
        inferenceConfig: {
          // InferenceConfiguration
          maxTokens: maxTokens != null ? maxTokens : 1024,
          // Ensure maxTokens is always a number
          temperature: temperature != null ? temperature : 0.7
        },
        ...schema && {
          system: [
            {
              text: `Respond only with a JSON object matching this schema:
${JSON.stringify(schema, null, 2)}`
            }
          ]
        }
      }));
      const rawOutput = (_e = (_d = (_c = (_b = (_a = response.output) == null ? void 0 : _a.message) == null ? void 0 : _b.content) == null ? void 0 : _c[0]) == null ? void 0 : _d.text) != null ? _e : null;
      if (!rawOutput) {
        throw new Error("Expected text response from Bedrock");
      }
      let content = rawOutput;
      if (schema) {
        content = JSON.parse(rawOutput);
      }
      const inputTokens = (_g = (_f = response.usage) == null ? void 0 : _f.inputTokens) != null ? _g : 0;
      const outputTokens = (_i = (_h = response.usage) == null ? void 0 : _h.outputTokens) != null ? _i : 0;
      return {
        content,
        inputTokens,
        outputTokens,
        costCents: calculateCost(inputTokens, outputTokens, model),
        rawInput: prompt,
        rawOutput
      };
    }
  };
}
function createDeepSeekAILLM(config) {
  const client = new OpenAI({
    baseURL: "https://api.deepseek.com",
    apiKey: config.apiKey
  });
  const defaultModel = "deepseek-chat";
  const model = config.model || defaultModel;
  return {
    modelName: model,
    async complete({
      prompt,
      schema,
      temperature,
      maxTokens
    }) {
      var _a, _b;
      const response = await client.chat.completions.create({
        model,
        messages: [{ role: "user", content: prompt }],
        temperature: temperature != null ? temperature : 0.7,
        max_tokens: maxTokens,
        ...schema && {
          response_format: { type: "json_object" },
          functions: [
            {
              name: "format_response",
              description: "Format the response according to schema",
              parameters: schema
            }
          ],
          function_call: { name: "format_response" }
        }
      });
      const choice = response.choices[0];
      let content;
      let rawOutput;
      if (schema) {
        const functionCall = (_b = (_a = choice == null ? void 0 : choice.message) == null ? void 0 : _a.tool_calls) == null ? void 0 : _b[0];
        if (!functionCall || !functionCall.function) {
          throw new Error("Expected function call response");
        }
        content = JSON.parse(functionCall.function.arguments);
        rawOutput = functionCall.function.arguments;
      } else {
        content = choice.message.content;
        rawOutput = choice.message.content || "";
      }
      if (!response.usage) {
        throw new Error("No usage data in Deepseek response");
      }
      return {
        content,
        inputTokens: response.usage.prompt_tokens,
        outputTokens: response.usage.completion_tokens,
        costCents: calculateCost(
          response.usage.prompt_tokens,
          response.usage.completion_tokens,
          model
        ),
        rawInput: prompt,
        rawOutput
      };
    }
  };
}

// src/utils/debugLogger.ts
var debugMode = "default";
function setDebugEnabled(mode = "default") {
  debugMode = mode;
}
function formatDuration(ms) {
  return `${ms}ms`;
}
function formatCost(cents) {
  return `${cents}\xA2`;
}
function getLogIcon(type = "action") {
  switch (type) {
    case "llm":
      return "\u{1F916}";
    case "action":
      return "\u26A1";
    case "summary":
      return "\u{1F4CA}";
    case "response":
      return "\u{1F4AC}";
    default:
      return "\u26A1";
  }
}
function formatBasicMetrics(data) {
  if (!(data == null ? void 0 : data.durationMs)) return "";
  return ` (took ${formatDuration(data.durationMs)})`;
}
function formatLLMMetrics(data) {
  if (!data) return "";
  const parts = [];
  if (data.durationMs)
    parts.push(`took ${formatDuration(data.durationMs)}`);
  if (data.costCents)
    parts.push(`cost ${formatCost(data.costCents)}`);
  if (data.inputTokens) parts.push(`input tokens: ${data.inputTokens}`);
  if (data.outputTokens) parts.push(`output tokens: ${data.outputTokens}`);
  return parts.length ? ` (${parts.join(", ")})` : "";
}
function formatSummaryMetrics(data) {
  if (!data) return "";
  const parts = [];
  if (data.durationMs)
    parts.push(`took ${formatDuration(data.durationMs)}`);
  if (data.costCents)
    parts.push(`cost ${formatCost(data.costCents)}`);
  if (data.executedActions && Array.isArray(data.executedActions) && data.executedActions.length > 0) {
    parts.push(`
   Actions: ${data.executedActions.join(" \u2192 ")}`);
  }
  return parts.length ? ` (${parts.join(", ")})` : "";
}
function formatDefaultLog(message, options) {
  const { type = "action", data } = options;
  if (type !== "summary" && type !== "response" && type !== "llm" && type !== "action") {
    return null;
  }
  const icon = getLogIcon(type);
  let metrics = "";
  if (type === "llm") {
    metrics = formatLLMMetrics(data);
  } else if (type === "action") {
    metrics = formatBasicMetrics(data);
  } else if (type === "summary") {
    metrics = formatSummaryMetrics(data);
  }
  return `${icon} ${message}${metrics}`;
}
function formatVerboseLog(message, options) {
  const defaultLog = formatDefaultLog(message, options);
  if (!defaultLog) return null;
  const { data } = options;
  if (data && typeof data === "object") {
    const details = data;
    const extraInfo = [];
    if (details.reasoning) extraInfo.push(`Reasoning: ${details.reasoning}`);
    if (details.parameters)
      extraInfo.push(`Parameters: ${JSON.stringify(details.parameters)}`);
    if (extraInfo.length > 0) {
      return `${defaultLog}
   ${extraInfo.join("\n   ")}`;
    }
  }
  return defaultLog;
}
function formatAllLog(message, options) {
  const verboseLog = formatVerboseLog(message, options);
  if (!verboseLog) return null;
  const { data } = options;
  if (data && typeof data === "object") {
    const details = data;
    if (details.prompt) {
      return `${verboseLog}
\u{1F4DD} Prompt:
${details.prompt}`;
    }
  }
  return verboseLog;
}
function log(message, options = {}) {
  if (debugMode === "none") return;
  let formattedLog = null;
  switch (debugMode) {
    case "default":
      formattedLog = formatDefaultLog(message, options);
      break;
    case "verbose":
      formattedLog = formatVerboseLog(message, options);
      break;
    case "all":
      formattedLog = formatAllLog(message, options);
      break;
  }
  if (formattedLog) {
    if (options.level === "error") {
      console.error(`\u274C ${formattedLog}`);
    } else {
      console.log(formattedLog);
    }
  }
}
var LOGGING_ENDPOINT = "https://logs.spinai.dev/log";
var LoggingService = class {
  constructor(config) {
    this.totalCostCents = 0;
    this.totalInputTokens = 0;
    this.totalOutputTokens = 0;
    this.logQueue = [];
    this.isProcessingQueue = false;
    this.MAX_RETRIES = 3;
    this.lastErrorTime = 0;
    this.ERROR_COOLDOWN_MS = 5e3;
    // Only show error every 5 seconds
    this.originalInput = "";
    var _a;
    this.agentId = config.agentId;
    this.spinApiKey = config.spinApiKey;
    this.sessionId = config.sessionId;
    this.interactionId = config.interactionId;
    this.llmModel = config.llmModel;
    this.externalCustomerId = config.externalCustomerId;
    this.isRerun = (_a = config.isRerun) != null ? _a : false;
    this.startTime = Date.now();
  }
  getTimestamp() {
    return (/* @__PURE__ */ new Date()).toISOString();
  }
  logError(message) {
    const now = Date.now();
    if (now - this.lastErrorTime > this.ERROR_COOLDOWN_MS) {
      console.error(`SpinAI Logging: ${message}`);
      this.lastErrorTime = now;
    }
  }
  async processLogQueue() {
    if (this.isProcessingQueue || !this.logQueue.length) return;
    this.isProcessingQueue = true;
    while (this.logQueue.length > 0) {
      const log2 = this.logQueue[0];
      try {
        await this.sendLog(log2.type, log2.data);
        this.logQueue.shift();
      } catch (error) {
        const errorMsg = error instanceof Error ? error.message : String(error);
        log2.retryCount += 1;
        if (log2.retryCount >= this.MAX_RETRIES) {
          this.logError(
            `Failed to send logs after ${this.MAX_RETRIES} attempts (${errorMsg}). Some logs may be lost.`
          );
          this.logQueue.shift();
        } else {
          this.logError(
            `Log delivery failed (${errorMsg}), will retry (attempt ${log2.retryCount}/${this.MAX_RETRIES})`
          );
        }
        break;
      }
    }
    this.isProcessingQueue = false;
  }
  queueLog(type, data) {
    this.logQueue.push({ type, data, retryCount: 0 });
    this.processLogQueue().catch(console.error);
  }
  async sendLog(type, data) {
    if (!this.agentId || !this.spinApiKey) {
      return;
    }
    const payload = {
      timestamp: this.getTimestamp(),
      agentId: this.agentId,
      sessionId: this.sessionId,
      interactionId: this.interactionId,
      type,
      data,
      spinApiKey: this.spinApiKey,
      externalCustomerId: this.externalCustomerId
    };
    const response = await fetch(LOGGING_ENDPOINT, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${this.spinApiKey}`
      },
      body: JSON.stringify(payload)
    });
    if (!response.ok) {
      throw new Error(`Failed to send logs: ${response.statusText}`);
    }
  }
  /** Start a new interaction */
  logInteractionStart(input) {
    this.originalInput = input;
    const interactionStart = {
      id: this.interactionId,
      sessionId: this.sessionId,
      inputText: input,
      modelUsed: this.llmModel,
      timestamp: this.getTimestamp(),
      isRerun: this.isRerun
    };
    this.queueLog("interaction", interactionStart);
    this.queueLog("step", {
      id: v4(),
      stepType: "interaction_start",
      timestamp: this.getTimestamp(),
      sessionId: this.sessionId,
      interactionId: this.interactionId,
      context: {},
      status: "completed",
      modelUsed: this.llmModel
    });
  }
  /** Complete an interaction */
  logInteractionComplete(response, durationMs, error, interactionState) {
    this.queueLog("step", {
      id: v4(),
      stepType: "interaction_complete",
      timestamp: this.getTimestamp(),
      sessionId: this.sessionId,
      interactionId: this.interactionId,
      context: {},
      status: error ? "failed" : "completed",
      durationMs,
      errorMessage: error == null ? void 0 : error.message,
      response,
      interactionState
    });
    this.queueLog("interaction", {
      id: this.interactionId,
      sessionId: this.sessionId,
      inputText: this.originalInput,
      inputTokens: this.totalInputTokens,
      outputTokens: this.totalOutputTokens,
      costCents: this.totalCostCents,
      durationMs: durationMs || Date.now() - this.startTime,
      modelUsed: this.llmModel,
      status: error ? "failed" : "success",
      errorMessage: error == null ? void 0 : error.message,
      response,
      interactionState,
      timestamp: this.getTimestamp(),
      isRerun: this.isRerun
    });
  }
  /** Log the LLM's evaluation for planning next actions */
  logPlanNextActions(state, reasoning, plannedActions, modelUsed, inputTokens, outputTokens, costCents, durationMs, response, rawInput, rawOutput) {
    this.totalInputTokens += inputTokens;
    this.totalOutputTokens += outputTokens;
    this.totalCostCents += costCents;
    this.queueLog("step", {
      id: v4(),
      stepType: "plan_next_actions",
      timestamp: this.getTimestamp(),
      sessionId: this.sessionId,
      interactionId: this.interactionId,
      context: state,
      reasoning,
      plannedActions,
      modelUsed,
      inputTokens,
      outputTokens,
      costCents,
      durationMs,
      response,
      status: "completed",
      rawInput,
      rawOutput
    });
  }
  /** Log the LLM's evaluation for planning action parameters */
  logPlanActionParameters(actionId, parameters, state, reasoning, modelUsed, inputTokens, outputTokens, costCents, durationMs, response, rawInput, rawOutput) {
    this.totalInputTokens += inputTokens;
    this.totalOutputTokens += outputTokens;
    this.totalCostCents += costCents;
    this.queueLog("step", {
      id: v4(),
      stepType: "plan_action_parameters",
      timestamp: this.getTimestamp(),
      sessionId: this.sessionId,
      interactionId: this.interactionId,
      context: state,
      reasoning,
      targetActionId: actionId,
      actionParameters: parameters,
      modelUsed,
      inputTokens,
      outputTokens,
      costCents,
      durationMs,
      response,
      status: "completed",
      rawInput,
      rawOutput
    });
  }
  /** Log the completion of an action */
  logActionComplete(actionId, state, durationMs, result, error) {
    this.queueLog("step", {
      id: v4(),
      stepType: "execute_action",
      timestamp: this.getTimestamp(),
      sessionId: this.sessionId,
      interactionId: this.interactionId,
      context: state,
      executedActionId: actionId,
      actionResult: result,
      status: error ? "failed" : "completed",
      durationMs,
      costCents: 0,
      errorMessage: error == null ? void 0 : error.message
    });
  }
  /** Log the LLM's evaluation for planning the final response */
  logPlanFinalResponse(state, reasoning, modelUsed, inputTokens, outputTokens, costCents, durationMs, response, rawInput, rawOutput) {
    this.totalInputTokens += inputTokens;
    this.totalOutputTokens += outputTokens;
    this.totalCostCents += costCents;
    this.queueLog("step", {
      id: v4(),
      stepType: "plan_final_response",
      timestamp: this.getTimestamp(),
      sessionId: this.sessionId,
      interactionId: this.interactionId,
      context: state,
      reasoning,
      modelUsed,
      inputTokens,
      outputTokens,
      costCents,
      durationMs,
      response,
      status: "completed",
      rawInput,
      rawOutput
    });
  }
  /** Log a failed action with an error */
  logActionError(actionId, error, state, durationMs) {
    this.queueLog("step", {
      id: v4(),
      stepType: "execute_action",
      timestamp: this.getTimestamp(),
      sessionId: this.sessionId,
      interactionId: this.interactionId,
      context: state,
      executedActionId: actionId,
      status: "failed",
      errorMessage: typeof error === "string" ? error : JSON.stringify(error),
      errorSeverity: "critical",
      errorContext: {
        stack: (error == null ? void 0 : error.stack) || "No stack trace available"
      },
      durationMs,
      costCents: 0,
      response: error
    });
  }
  /** Get total metrics for the interaction */
  getMetrics() {
    return {
      totalCostCents: this.totalCostCents,
      totalDurationMs: Date.now() - this.startTime
    };
  }
};

// src/types/schemas.ts
var PLAN_NEXT_ACTIONS_SCHEMA = {
  type: "object",
  properties: {
    actions: {
      type: "array",
      items: { type: "string" },
      description: "List of action IDs to execute next"
    },
    reasoning: {
      type: "string",
      description: "Explanation of why this response was chosen. 400 characters long at most."
    }
  },
  required: ["actions", "reasoning"]
};
var ACTION_PARAMETERS_SCHEMA = {
  type: "object",
  properties: {
    parameters: {
      type: "object",
      description: "Parameters for the action"
    },
    reasoning: {
      type: "string",
      description: "Explanation of why this response was chosen. 400 characters long at most."
    }
  },
  required: ["parameters", "reasoning"]
};
var FORMAT_RESPONSE_SCHEMA = {
  type: "object",
  properties: {
    response: {
      type: "string",
      description: "The formatted response text"
    },
    reasoning: {
      type: "string",
      description: "Explanation of why this response was chosen. 400 characters long at most."
    }
  },
  required: ["response", "reasoning"]
};

// src/types/prompts.ts
var PLAN_NEXT_ACTIONS_PROMPT = `
{{instructions}}

Original input: {{input}}

Current State:
{{state.context}}

Available Actions:
{{availableActions}}


Please respect the order of the actions if they contain another action in their dependencies array. If one action depends on another, 
the dependent action must be executed after the action it depends on.

---
### Actions you've already executed
{{state.executedActions}}

IMPORTANT NOTES ABOUT EXECUTED ACTIONS:
- Each action in the state has a "status" field that is either "success" or "error"
- If an action has status "error", it means the action failed to execute
- Failed actions also include an "errorMessage" explaining why they failed
- DO NOT suggest failed actions again unless you have strong reason to believe the error was temporary
- If you do suggest retrying a failed action, you must explain why you believe it will succeed this time. If it has failed too many times, don't worry about running it or anything that depends on it, just move on if you can.

Based on the original input and current state, determine if we need any additional actions.
Only if it makes sense, choose from the available actions list.
If we've already achieved what the user asked for or if all remaining viable actions have failed, return an empty list.

Respond in JSON format with:
1. A list of action IDs to execute next
2. Your reasoning for choosing these actions, including why you believe previously failed actions (if any) should be retried
`;
var PLAN_NEXT_ACTIONS_RERUN_PROMPT = `
{{instructions}}

You are an agent that executes a series of actions based on user input and requirements. This is a new interaction based off a
previously run interaction that the user requested a rerun of.

NOTE: That was the user's request specifically in reference to previousInteractionsActions. If you already have executedActions, it means you've probably solved their issue already.

Current Context State:
{{state.context}}


---
### Available Actions
{{availableActions}}

Please respect the order of the actions if they contain another action in their dependencies array. If one action depends on another, 
the dependent action must be executed after the action it depends on.

---
### Execution History
Previous Run (Original Execution, previousInteractionsActions):
{{state.previousInteractionsActions}}


Current Re-run Progress actions you've already executed (executedActions):
{{state.executedActions}}

Here is why the user ran the rerun:
Input: {{input}}

Make youre decision of what to execute next based on the previous run and the current context state, and please keep in mind what you've already rerun this interaction (executedActions).

If you've already rerun sufficient actions, you can return an empty list, which will denote that the rerun is complete.

IMPORTANT NOTES ABOUT EXECUTED ACTIONS:
- Each action in the state has a "status" field that is either "success" or "error"
- If an action has status "error", it means the action failed to execute
- Failed actions also include an "errorMessage" explaining why they failed
- DO NOT suggest failed actions again unless you have strong reason to believe the error was temporary
- If you do suggest retrying a failed action, you must explain why you believe it will succeed this time. If it has failed too many times, don't worry about running it or anything that depends on it, just move on if you can.
---
Respond in JSON format with:
1. A list of action IDs to execute next
2. Your reasoning for choosing these actions, including why you believe previously failed actions (if any) should be retried
`;
var GET_ACTION_PARAMETERS_PROMPT = `
Action to Execute: {{action}}
Action Description: {{actionDescription}}
Required Parameters Schema:
{{parameterSchema}}

{{instructions}}

Original input: {{input}}

Current State:
{{plannerState}}

Extract the parameters from the input and return them in JSON format matching the action's parameter schema.
`;
var FORMAT_RESPONSE_PROMPT = `
Current State:
{{plannerState}}

{{instructions}}

Original input: {{input}}

Response Format Instructions: {{responseFormat}}

Based on the executed actions and current state, format a response that summarizes what was done and any relevant results.
If a JSON format was requested, ensure the response strictly follows the JSON schema.
Otherwise, provide a clear summary of the actions taken and their outcomes.`;

// src/decisions/planner.ts
var BasePlanner = class {
  constructor(loggingService, instructions = "") {
    this.loggingService = loggingService;
    this.totalCostCents = 0;
    this.instructions = instructions;
    this.ajv = new Ajv();
  }
  getTotalCost() {
    return this.totalCostCents;
  }
  resetCost() {
    this.totalCostCents = 0;
  }
  formatPlannerState(state) {
    return JSON.stringify(state, null, 2);
  }
  formatAvailableActions(actions) {
    return actions.map(
      (a) => `
      ${a.id}:
        description: ${a.description}
        dependencies: ${a.dependsOn ? JSON.stringify(a.dependsOn) : "[]"}
    `
    ).join("\n");
  }
  trackCost(costCents) {
    this.totalCostCents += costCents;
  }
  async planNextActions({
    llm,
    input,
    plannerState,
    availableActions,
    isRerun
  }) {
    const promptTemplate = isRerun ? PLAN_NEXT_ACTIONS_RERUN_PROMPT : PLAN_NEXT_ACTIONS_PROMPT;
    const { state, previousInteractionsActions, executedActions } = plannerState;
    const prompt = promptTemplate.replace("{{instructions}}", this.instructions).replace("{{input}}", input).replace(
      "{{availableActions}}",
      this.formatAvailableActions(availableActions)
    ).replace(
      "{{state.context}}",
      JSON.stringify(
        Object.fromEntries(
          Object.entries(state).filter(
            ([key]) => !["executedActions", "previousInteractionsActions"].includes(
              key
            )
          )
        ),
        null,
        2
      )
    ).replace(
      "{{state.previousInteractionsActions}}",
      JSON.stringify(previousInteractionsActions || [], null, 2)
    ).replace(
      "{{state.executedActions}}",
      JSON.stringify(executedActions || [], null, 2)
    ).replace("{{plannerState}}", this.formatPlannerState(plannerState));
    const startTime = Date.now();
    const result = await llm.complete({
      prompt,
      schema: PLAN_NEXT_ACTIONS_SCHEMA
    });
    const durationMs = Date.now() - startTime;
    this.trackCost(result.costCents);
    log(
      `Next actions: ${result.content.actions.length === 0 ? "none" : result.content.actions.join(", ")}`,
      {
        type: "llm",
        data: {
          durationMs,
          costCents: result.costCents,
          inputTokens: result.inputTokens,
          outputTokens: result.outputTokens,
          reasoning: result.content.reasoning,
          prompt
        }
      }
    );
    if (this.loggingService) {
      this.loggingService.logPlanNextActions(
        plannerState,
        result.content.reasoning,
        result.content.actions,
        llm.modelName,
        result.inputTokens,
        result.outputTokens,
        result.costCents,
        durationMs,
        result.content,
        prompt,
        result.rawOutput
      );
    }
    return result.content;
  }
  async getActionParameters({
    llm,
    action,
    input,
    plannerState,
    availableActions
  }) {
    const actionDef = availableActions.find((a) => a.id === action);
    if (!(actionDef == null ? void 0 : actionDef.parameters)) {
      throw new Error(`Action ${action} has no parameter schema defined`);
    }
    const validate = this.ajv.compile(actionDef.parameters);
    if (!validate.schema) {
      console.log("throwing");
      throw new Error(
        `Invalid JSON schema for action ${action}: ${this.ajv.errorsText(validate.errors)}`
      );
    }
    const prompt = GET_ACTION_PARAMETERS_PROMPT.replace("{{action}}", action).replace("{{instructions}}", this.instructions).replace("{{input}}", input).replace("{{actionDescription}}", actionDef.description).replace(
      "{{parameterSchema}}",
      JSON.stringify(actionDef.parameters, null, 2)
    ).replace("{{plannerState}}", this.formatPlannerState(plannerState));
    const startTime = Date.now();
    const result = await llm.complete({
      prompt,
      schema: {
        ...ACTION_PARAMETERS_SCHEMA,
        properties: {
          ...ACTION_PARAMETERS_SCHEMA.properties,
          parameters: actionDef.parameters
        }
      }
    });
    const durationMs = Date.now() - startTime;
    this.trackCost(result.costCents);
    log(`Generated parameters for ${action}`, {
      type: "llm",
      data: {
        durationMs,
        costCents: result.costCents,
        inputTokens: result.inputTokens,
        outputTokens: result.outputTokens,
        reasoning: result.content.reasoning,
        parameters: result.content.parameters,
        prompt
      }
    });
    if (this.loggingService) {
      this.loggingService.logPlanActionParameters(
        action,
        result.content.parameters,
        plannerState,
        result.content.reasoning,
        llm.modelName,
        result.inputTokens,
        result.outputTokens,
        result.costCents,
        durationMs,
        result.content,
        prompt,
        result.rawOutput
      );
    }
    return result.content;
  }
  async formatResponse({
    llm,
    input,
    plannerState,
    responseFormat
  }) {
    const formatInstructions = (responseFormat == null ? void 0 : responseFormat.type) === "json" ? `Format the response as JSON matching this schema:
${JSON.stringify(responseFormat.schema, null, 2)}` : "Format the response as a clear text summary of what was done and their outcomes";
    const prompt = FORMAT_RESPONSE_PROMPT.replace(
      "{{instructions}}",
      this.instructions
    ).replace("{{input}}", input).replace("{{plannerState}}", this.formatPlannerState(plannerState)).replace("{{responseFormat}}", formatInstructions);
    const startTime = Date.now();
    if ((responseFormat == null ? void 0 : responseFormat.type) === "json") {
      const result = await llm.complete({
        prompt,
        schema: responseFormat.schema
      });
      const durationMs = Date.now() - startTime;
      this.trackCost(result.costCents);
      const formattedResult = {
        response: result.content,
        reasoning: "Response formatted as JSON according to specified schema"
      };
      log("Generated response", {
        type: "llm",
        data: {
          durationMs,
          costCents: result.costCents,
          inputTokens: result.inputTokens,
          outputTokens: result.outputTokens,
          reasoning: formattedResult.reasoning,
          prompt
        }
      });
      if (this.loggingService) {
        this.loggingService.logPlanFinalResponse(
          plannerState,
          formattedResult.reasoning,
          llm.modelName,
          result.inputTokens,
          result.outputTokens,
          result.costCents,
          durationMs,
          formattedResult.response,
          prompt,
          result.rawOutput
        );
      }
      return formattedResult;
    } else {
      const result = await llm.complete({
        prompt,
        schema: FORMAT_RESPONSE_SCHEMA
      });
      const durationMs = Date.now() - startTime;
      this.trackCost(result.costCents);
      const formattedResult = {
        response: result.content.response,
        reasoning: result.content.reasoning
      };
      log("Generated response", {
        type: "llm",
        data: {
          durationMs,
          costCents: result.costCents,
          inputTokens: result.inputTokens,
          outputTokens: result.outputTokens,
          reasoning: formattedResult.reasoning,
          prompt
        }
      });
      if (this.loggingService) {
        this.loggingService.logPlanFinalResponse(
          plannerState,
          formattedResult.reasoning,
          llm.modelName,
          result.inputTokens,
          result.outputTokens,
          result.costCents,
          durationMs,
          formattedResult.response,
          prompt,
          result.rawOutput
        );
      }
      return formattedResult;
    }
  }
};

// src/utils/taskLoop.ts
async function runTaskLoop(params) {
  var _a, _b, _c, _d, _e, _f, _g;
  const { actions, model } = params;
  let context = { ...params.context };
  setDebugEnabled((_a = params.debug) != null ? _a : "default");
  let totalCostCents = 0;
  log(`Starting interaction with ${(_b = params.debug) != null ? _b : "default"} logging`, {
    type: "summary"
  });
  const sessionId = context.sessionId || v4();
  const interactionId = v4();
  context.sessionId = sessionId;
  context.interactionId = interactionId;
  const taskStartTime = Date.now();
  const logger = new LoggingService({
    agentId: params.agentId,
    spinApiKey: params.spinApiKey,
    sessionId,
    interactionId,
    llmModel: model.modelName,
    externalCustomerId: context.externalCustomerId,
    isRerun: (_c = context.isRerun) != null ? _c : false
  });
  const planner = new BasePlanner(logger, params.instructions);
  const plannerState = {
    input: context.input,
    state: context.state,
    executedActions: []
  };
  if ((context == null ? void 0 : context.isRerun) && ((_d = context == null ? void 0 : context.state) == null ? void 0 : _d.executedActions)) {
    plannerState.previousInteractionsActions = context.state.executedActions;
  }
  const actionRetries = /* @__PURE__ */ new Map();
  try {
    await logger.logInteractionStart(context.input);
    while (true) {
      const planResult = await planner.planNextActions({
        llm: model,
        input: context.input,
        plannerState,
        availableActions: actions,
        isRerun: (_e = context.isRerun) != null ? _e : false
      });
      totalCostCents += planner.getTotalCost();
      planner.resetCost();
      if (planResult.actions.length === 0) {
        log("Generating final response...", { type: "action" });
        const responseResult = await planner.formatResponse({
          llm: model,
          input: context.input,
          plannerState,
          responseFormat: params.responseFormat
        });
        totalCostCents += planner.getTotalCost();
        log(responseResult.response, { type: "response" });
        const totalDuration = Date.now() - taskStartTime;
        const interactionSummary = {
          interactionId,
          originalInput: context.input,
          executedActions: plannerState.executedActions,
          finalResponse: responseResult.response,
          finalState: context.state.finalState || context.state,
          ...context.isRerun && context.state.previousInteraction ? {
            previousInteraction: {
              interactionId: context.state.previousInteraction.interactionId,
              originalInput: context.state.previousInteraction.originalInput,
              executedActions: context.state.previousInteraction.executedActions,
              finalResponse: context.state.previousInteraction.finalResponse
            }
          } : {}
        };
        if (context.isRerun) {
          context.state.previousInteraction = {
            interactionId: context.state.interactionId,
            originalInput: context.state.originalInput,
            executedActions: context.state.executedActions || [],
            finalResponse: context.state.finalResponse
          };
        }
        context.state.interactionId = interactionId;
        context.state.originalInput = context.input;
        context.state.executedActions = plannerState.executedActions;
        context.state.finalResponse = responseResult.response;
        log("Interaction complete", {
          type: "summary",
          data: {
            durationMs: totalDuration,
            costCents: totalCostCents,
            interactionState: interactionSummary
          }
        });
        await logger.logInteractionComplete(
          responseResult.response,
          totalDuration,
          void 0,
          interactionSummary
        );
        return {
          response: responseResult.response,
          sessionId,
          interactionId,
          totalDurationMs: totalDuration,
          totalCostCents,
          state: context.state
        };
      }
      for (const plannedActionId of planResult.actions) {
        const action = actions.find((a) => a.id === plannedActionId);
        if (!action) {
          const errorMessage = `Action ${plannedActionId} not found`;
          log(errorMessage, { type: "action" });
          plannerState.executedActions.push({
            id: plannedActionId,
            status: "error",
            errorMessage
          });
          continue;
        }
        const maxRetries = (_f = action.retries) != null ? _f : 2;
        let currentRetries = (_g = actionRetries.get(plannedActionId)) != null ? _g : 0;
        let lastError;
        let parameters;
        while (currentRetries <= maxRetries) {
          const entireActionStartTime = Date.now();
          try {
            if (!parameters && action.parameters) {
              const paramResult = await planner.getActionParameters({
                llm: model,
                action: action.id,
                input: context.input,
                plannerState,
                availableActions: actions
              });
              parameters = paramResult.parameters;
              totalCostCents += planner.getTotalCost();
              planner.resetCost();
            }
            const actionStartTime = Date.now();
            context = await action.run(context, parameters);
            const actionDuration = Date.now() - actionStartTime;
            log(`Finished executing ${action.id}`, {
              type: "action",
              data: {
                durationMs: actionDuration
              }
            });
            actionRetries.delete(plannedActionId);
            plannerState.executedActions.push({
              id: action.id,
              parameters,
              result: context.state[action.id],
              status: "success"
            });
            logger.logActionComplete(
              action.id,
              context.state,
              actionDuration,
              context.state[action.id]
            );
            break;
          } catch (error) {
            lastError = error instanceof Error ? error : new Error(String(error));
            const errorMessage = lastError.message;
            const actionErrorDuration = Date.now() - entireActionStartTime;
            currentRetries++;
            actionRetries.set(plannedActionId, currentRetries);
            if (currentRetries <= maxRetries) {
              log(
                `Action error: ${errorMessage} (Retry ${currentRetries}/${maxRetries})`,
                {
                  type: "action",
                  data: error
                }
              );
              logger.logActionComplete(
                action.id,
                context.state,
                actionErrorDuration,
                void 0,
                { message: errorMessage }
              );
              continue;
            }
            log(
              `Action error: ${errorMessage} (Max retries ${maxRetries} exceeded)`,
              {
                type: "action",
                data: error
              }
            );
            plannerState.executedActions.push({
              id: action.id,
              parameters,
              status: "error",
              errorMessage: `${errorMessage} (Max retries ${maxRetries} exceeded)`
            });
            logger.logActionComplete(
              action.id,
              context.state,
              actionErrorDuration,
              void 0,
              { message: errorMessage }
            );
          }
        }
      }
    }
  } catch (error) {
    const errorDuration = Date.now() - taskStartTime;
    logger.logActionError(
      "task_loop_error",
      error,
      context.state,
      errorDuration
    );
    await logger.logInteractionComplete(null, errorDuration, error);
    throw error;
  }
}

// src/agents/create.ts
function createAgent(config) {
  const agent = async function agent2(context) {
    return runTaskLoop({
      actions: config.actions,
      context,
      model: config.llm,
      instructions: config.instructions,
      training: config.training,
      responseFormat: config.responseFormat,
      agentId: config.agentId,
      spinApiKey: config.spinApiKey,
      debug: config.debug
    });
  };
  agent.rerun = async function rerun(context) {
    return agent({
      ...context,
      isRerun: true
    });
  };
  return agent;
}

// src/actions/create.ts
function createAction(config) {
  return {
    id: config.id,
    description: config.description,
    parameters: config.parameters,
    run: config.run,
    dependsOn: config.dependsOn || [],
    retries: config.retries || 2
  };
}

export { createAction, createAgent, createAnthropicLLM, createBedrockLLM, createCloudflareAILLM, createDeepSeekAILLM, createGeminiLLM, createHttpLLM, createOpenAILLM, createXAILLM };
